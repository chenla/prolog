#   -*- mode: org; fill-column: 60 -*-
#+TITLE: Cognition
#+STARTUP: showall
#+PROPERTY: filename
  :PROPERTIES:
  :Name: /home/deerpig/proj/chenla/prolog/prolog-cognition.org
  :Created: 2017-04-15T19:17@Prek Leap (11.642600N-104.919210W)
  :ID: 4ba0d72f-c953-4d16-a197-55d3910de63e
  :URL:
  :END:
#+INCLUDE: ./inc/head.org
#+INCLUDE: ./inc/macros.org

{{{breadcrumbs}}}

#+HTML: <div class="outline-2" id="meta">
| *Author*  | Brad Collins <brad@chenla.org>             |
| *Date*    | {{{time(%Y-%m-%d %H:%M:%S)}}}              |
#+HTML: </div>

#+TOC: headlines 4

* Introduction

* cognition as territory

It came to me last night, just as I was ready to close up
the office, that the reason that intelligence is a black
box is because in the map-territory relation, once a
cognitive system reaches a certain threshold of complexity
it can not longer be legible.

You can't separate intelligence from the information that it
learned from.  Humans are not naturally intelligent, when we
are born we have some preprogrammed behavior but other than
that there isn't much there.  But as babies observe the
world, they start to identify patterns from sensory
information and from experimenting with that information and
finding out what works and what doesn't.  We don't become
intelligent, and perhaps even self-conscious until we have
ingested enough information to understand what we are and
some concept of what the world is like.  So the training
data set is what we take in, crunch it up and become
intelligent.  But what happens to that training data is
unknown.  We don't record like a camera or microphone, we
create maps of patterns that become memories -- and memories
are reconstructions of sensory events, that are likely never
really ever the same twice.  We constantly reinventing our
memories relative to the present.

The other point to make is that intelligence has to include
not just what happened, but what we think might happen and
how probable those outcomes may be.  So our mind is not a
storehouse of facts, it is in fact a fnord, embracing a wide
range of posibilities.  In in that sense, every mind is made
up of a large number of interconneted fnords which
constitute a territory in it's own right.  Our brains are
not maps of the world, though we are constantly building and
maintaining a model of the world.  But that model is not a
map it's a territory -- and territories are not maps.  Maps
simplify and make territories legible.

So intelligent systems will never be legible in the way we
would like because they are unintelligible and illegible
worlds.  This is messy, no question about it.  And it will
be frustrating, because it will be true of machine
intelligences no less than it is of meat intelligences.
It's sort of the uncertainty principle for Intelligence.

This is not to say that we won't eventually be able to ask
AIs why they came up with an answer, or why they did
something but they will have to do it the way we did, by
coming up with a plausible reason that fits the facts.  That
doesn't mean that this is really why or how, but it will
have to be good enough.  And when it does, the AI will be
rationalizing its actions, building an argument for why and
how, rather than replaying what it did like a video that
explains step by step it's detailed reasoning.  There will
be times that it knows that it based its action on the
probability of x y z outcomes that were used to create a
scenario -- but there will be all sorts of things that will
/feel/ like the right solution and will go with that, but
not be able to explain why it felt right.

This theory, if it pans out will be yet another nail in the
human exceptionalist coffin and will make a number of people
deeply uncomfortable.

This is why things like BMF is so important, AIs will need
to externally document what they find, just like we do.
Their minds will not be recording devices, they will need to
externalize their thoughts, and experiences and use other
dumb compuational computers (cloud) to do specialized number
crunching.  Take this to the next step, and like Kevin Kelly
envisions, every intelligence will be different, there will
be no one general AI that does everything.  AIs will have to
google things, and they will have to compose their thoughts
like we do, as way of learning and understanding.  Our AIs
will be a transactive species like us -- and will relying on
each other and us, just we rely on each other and them.
Each is a holon, and together we are a whole.

The Borg are simply a distributed cloud -- and the cloud is
an intelligence.  But when humans embrace AIs as an
extension of themselves and a part of the larger concept of
mankind then we are all holons and we all form larger wholes
which are part of larger holons.

The other thing that I was thinking about last night is how
to measure intelligence -- hell, how does one even /define/
intelligence?  One way to look at it is, again to think of
it as a fnord -- the smarter you are, the more causal chains
you can pick out in a each collection of fnords.  One
assertion may have many different possible causal chains,
but when you start adding more and more fnords, the
complexity of how many causal chains that are possible
quickly become overwhelming.  Perhaps the smarter you are,
the better or broader you ability to integrate more and more
causal chains so that your mind -- which is a world, grows
and becomes more complex.  I'm not articulating this very
well at the moment becuase I'm still trying to explore the
concept.  But it is an intriguing idea.

A calculator is dumb as paint -- it takes inputs and does a
calculation and that's that.  But a bit of code with an if
then statement actually thinks -- it takes an input and
evaluates between two or more possibilities and comes up
with an answer -- the if then statement embraces a number of
different causal inputs.

If we create a bunch of if then statements that run in
parallel, so that each is looking for something different we
have something even more interesting.  So if we have a pile
of fruit, and then chunks of code which each are looking for
a different kind of fruit, and the same input is then sent
to all the fruit identifiers, you might get one of them that
says -- ah, an apple!  The others might say, alas, not a
pinapple, or blast it's not a mango, or damn, it's not a
rambutan.  But that's getting ahead of ourselves.  We first
need pattern matchers to figure out if what it is looking at
is a fruit or something else like a rock.  So you might have
a level of parallel pattern matchers that are looking for
things that organic and edible and grow on trees.  At each
level several pattern matchers will fire off 'I found it'
and pattern matchers at the level above who are waiting for
things that in turn tell them they found what they are
looking for.  So that eventually, once you know it's a
fruit, the color layer will say I found yellow, and the ripe
mango and banana matches above say they found yellow fruits.
But only the banana matcher will also see that's the yellow
fruit is long and faceted and in a bunch.  Which the mango
matcher will ignore and say 'doh'.

I only explain all of this because this is how our neocortex
works as well as machine learning software.  And in each
case we have a pattern matcher that works a little like an
if then statement -- that is aware of a range of different
possible things.  Taken all of these pattern matchers
together working in parallel and we have a system which
quickly becomes far more complex than the sum of its parts.

Add to that, that every new input that goes through the
matching tree, also improves and changes individual
patterns.  Each matcher that finds a positive or a negative
is strengthened and becomes more complex and confident in
what it is looking at.  This means that the system is not
the same as a traditional piece of software that is run and
will do the same thing every time it is run -- a machine
learning system is always growing and changing.  That, in
effect is why it is a territory and not a map.  In other
words, you can never step in the same river twice, and by
the time you ask the river what it was, it is no longer the
same river and can not answer because it does not know.




* The Black Box

Our brains have always been black boxes -- just like the
emerging AI systems we are building.  Stop worrying about
not knowing what happens inside the black box so much as
worrying about how the black boxes will externalize and
communicate and remember the shit that comes out of the box.
Treat AI's as the same as our brains and construct tools
that extend those black boxes so that other AIs and humans
can work out what is good and bad and bullshit and the rest.
The collective is the best means of checking and balancing
-- not handicapping.  Shit goes in and shit goes out and you
deal with what comes out and don't work so much about what
happens in between.  If someone goes off the rails then they
are cut out.  It's the system that has to regulate -- there
won't be any one AI that could dominate and do what people
are scared of at the moment, unless we don't even try to pay
attention to the shit that comes out.  This is not magic,
it's how everything works in nature.  It's why scientists
who work in biotech aren't worried about gray goo -- nature
is far more robust than we think -- and that's how we have
to build AI.

Perhaps that isn't clear -- brains kept externalizing by
building other brains around them, and then we externalized
outside the body with spoken language, then written
language, the external cognition and external muscle.

Each of our brains are black boxes to the the other parts of
our brains.

AI's will externalize in the same way -- by building layers
on layers that don't replace the layers beneath them.  These
layers should in principle work as pace layers when they
work, and shearing layers when they don't.

We assume that AIs will be able to replace themselves
completely -- but that can't happen because it's not the
same as creating whole new brains that replace the old one
-- there is no bridge to make that happen, so we will end up
with crufty AIs that will be complex nested black boxes that
will still have to talk to other black boxes and we're back
to the same messy situation that humans have to deal with
today.


* Complex systems as a black box


It might well be that all /intelligences/, because they are
complex systems, will always be a black box, human, machine,
whatever.  That that's the deal.  What goes in, might be
legible, what comes out, might be legible as well.  But the
complexity makes whatever happens inbetween illegible and
that if you try to take it apart and make sense of it, you
will just get a sum of it's parts, not a legible
intelligance.  The AIs we are building are ourselves.
That's the bargin and we might just have to live with it.

Can this be proved?  That intelligences are illegible?

If it turns out that human intelligence is not just
happening at the biological level, but that there are 
quantum effects that are involved... then the answer is
yes.  And it likely won't be as difficult as we think to
build machine intelligence that does the same thing in a
different way.  But what if it's not?  What if it is all
electro-chemical computation?  It still might not be
knowable.  Just as we can't know the weather -- once you
cross a complexity threshold it becomes a black box.


* Molecular Biology is Hard

#+begin_quote
Because molecular biology is wayyyyyyyyyyyyyy harder than
non-biologists realize, in large part due to people thinking
'genetic code' means something like software code, when it
was intended to be analogous to cryptographic codes instead.

The human genome is not the source code for the human body,
but rather a parts list, and an incomplete one at
that. Unfortunately, it's encrypted. Fortunately, we broke
the code 50 years ago. Unfortunately, it was also written in
Klingon. We've spent 50 years trying to translate it
(determine protein crystal/NMR structures), and
simultaneously trying to figure out how the parts go
together. We're maybe 20% through with the
translation. We’re much further behind on figuring out how
it actually works. Completing the translation of the parts
list would be helpful, but it’s no panacea.

The list of what we don’t know (and can’t predict from
protein structures alone) is far larger than what we do
know. Which proteins are expressed in which cells? Which
proteins interact with each other? When do they interact
with each other? How strong are those interactions? What
non-protein molecules do they make, and in what
concentrations? And keep in mind that each and every one of
those questions affects the others, often in ways that make
no freaking sense, because evolution is dumb.

As for protein structure prediction, maybe we’ll get there
eventually, but I’m skeptical; de novo prediction really
hasn’t made much progress in recent years. Computational
methods are still terrible at the (to my mind) much simpler
problem of predicting if/how drugs bind to known protein
structures, which does not make me optimistic. We’re pretty
good at predicting structures through homology, mind you,
but that’s a much simpler problem than going straight from
the amino acid sequence.

To get a broader sense of why biologists tend to be
skeptical that computational modeling can replace
experimental biology any time soon, see [[http://blogs.sciencemag.org/pipeline/archives/2017/04/28/software-eats-the-world-but-biology-eats-it][this recent piece]]
and the longer article that it links to.

-- [[https://www.reddit.com/user/zmil][zmil]] (comment)
   [[https://www.reddit.com/r/slatestarcodex/comments/688g0a/the_ai_cargo_cult_kevin_kellys_skepticism_of/]['The AI Cargo Cult': Kevin Kelly's skepticism of superhuman AI]] | slatestarcodex
#+end_quote
